{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BÃ i 4"
      ],
      "metadata": {
        "id": "Z7NVx7iG_So9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls / /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX0YL7s3Ca3r",
        "outputId": "ec65857f-690d-4446-83e7-7b2639d1a3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/:\n",
            "bin\t\t\t    dev     lib32   NGC-DL-CONTAINER-LICENSE  root  tmp\n",
            "boot\t\t\t    etc     lib64   opt\t\t\t      run   tools\n",
            "content\t\t\t    home    libx32  proc\t\t      sbin  usr\n",
            "cuda-keyring_1.0-1_all.deb  kaggle  media   python-apt\t\t      srv   var\n",
            "datalab\t\t\t    lib     mnt     python-apt.tar.xz\t      sys\n",
            "\n",
            "/:\n",
            "bin\t\t\t    dev     lib32   NGC-DL-CONTAINER-LICENSE  root  tmp\n",
            "boot\t\t\t    etc     lib64   opt\t\t\t      run   tools\n",
            "content\t\t\t    home    libx32  proc\t\t      sbin  usr\n",
            "cuda-keyring_1.0-1_all.deb  kaggle  media   python-apt\t\t      srv   var\n",
            "datalab\t\t\t    lib     mnt     python-apt.tar.xz\t      sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import cv2\n",
        "from glob import glob\n",
        "from termcolor import colored\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "J3-uvrhS7xuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
        "  def init_config_yolo(slef,class_mapping,S=7,B=2,C=20,custom_trnasforms=None):\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.class_mapping = class_maping\n",
        "    self.custom_transforms = custom_transforms\n",
        "  def __getitem__(self,index):\n",
        "    image,target = super(CustomVOCDataset,self).__getitem__(index)\n",
        "    img_width,img_height =image.size\n",
        "\n",
        "    bboxes = convert_to_yolo_format(target,img_width,img_height,self.class_mapping)\n",
        "    just_boxes = boxes[:,1:]\n",
        "    labels = boxes[:,0]\n",
        "    if self.custom_transforms:\n",
        "      sample = {\n",
        "          \"image\":np.array(image),\n",
        "          \"bboxes\":just_boxes,\n",
        "          \"labels\":labels\n",
        "      }\n",
        "      sample = self.custom_transforms(**sample)\n",
        "      image = sample['image']\n",
        "      boxes = sample['bboxes']\n",
        "      labels= sample['labels']\n",
        "    label_matrix = torch.zeros((self.S,self.S,self.C+5*self.B))\n",
        "\n",
        "    boxes = torch.tensor(boxes,dtype=torch.float32)\n",
        "    labels = torch.tensor(labels,dtype=torch.float32)\n",
        "    image = torch.as_tensor(image,dtype=torch.float32)\n",
        "\n",
        "    for box,class_label in zip(boxes,labels):\n",
        "      x,y,width,height = box.tolist()\n",
        "      class_label = int(class_label)\n",
        "\n",
        "      i,j = int(self.S*y), int(self.S*y)\n",
        "\n",
        "      x_cell,y_cell = self.S* x - j,self.S*y-i\n",
        "\n",
        "      width_cell,height_cell = (width*self.S,height*self.S)\n",
        "\n",
        "      if label_matrix[i,j,20] == 0:\n",
        "        label_matrix[i,j,20] =1\n",
        "\n",
        "        box_coordinates = torch.tensor([\n",
        "            x_cell,y_cell,width_cell,height_cell\n",
        "        ])\n",
        "\n",
        "        label_matrix[i,j,21:25] = box_coordinates\n",
        "\n",
        "        label_matrix[i,j,class_label] = 1\n",
        "    return image,label_matrix\n",
        "def convert_to_yolo_format(target,img_width,img_height,class_mapping):\n",
        "\n",
        "\n",
        "  annotations =target['annotation']['object']\n",
        "\n",
        "  real_width = int(target['annotation']['size']['width'])\n",
        "  real_height = int(target['annotation']['size']['height'])\n",
        "\n",
        "  if not isinstance(annotations,list):\n",
        "    annotations = [annotations]\n",
        "  boxes = []\n",
        "\n",
        "  for anno in annotations:\n",
        "    xmin = int(anno['bndbox']['xmin'])/real_width\n",
        "    xmax = int(anno['bndbox']['xmax'])/real_width\n",
        "    ymin = int(anno['bndbox']['ymin'])/real_height\n",
        "    ymax = int(anno['bndbox']['ymax'])/real_height\n",
        "\n",
        "    x_center = (xmin+xmax)/2\n",
        "    y_center = (ymin+ymax)/2\n",
        "    width  = xmax-xmin\n",
        "    height = ymax-ymin\n",
        "\n",
        "    class_name=anno['name']\n",
        "    class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
        "\n",
        "    boxes.append([class_id,x_center,y_center,width,height])\n",
        "  return np.array(boxes)"
      ],
      "metadata": {
        "id": "G7tBHHZU8M70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_preds,boxes_labels,box_format=\"midpoit\"):\n",
        "\n",
        "  if box_format == \"midpoint\":\n",
        "    box1_x1 = boxes_preds[...,0:1]-boxes_preds[...,2:3]/2\n",
        "    box1_y1 = boxes_preds[...,1:2]-boxes_preds[...,3:4]/2\n",
        "    box1_x2 = boxes_preds[...,0:1]+boxes_preds[...,2:3]/2\n",
        "    box1_y2 = boxes_preds[...,1:2]+boxes_preds[...,3:4]/2\n",
        "\n",
        "    box2_x1 = boxes_labels[...,0:1]-boxes_labels[...,2:3]/2\n",
        "    box2_y1 = boxes_labels[...,1:2]-boxes_labels[...,3:4]/2\n",
        "    box2_x2 = boxes_labels[...,0:1]+boxes_labels[...,2:3]/2\n",
        "    box2_y2 = boxes_labels[...,1:2]+boxes_labels[...,3:4]/2\n",
        "\n",
        "  if box_format == \"corners\":\n",
        "      box1_x1 = boxes_preds[...,0:1]\n",
        "      box1_y1 = boxes_preds[...,1:2]\n",
        "      box1_x2 = boxes_preds[...,2:3]\n",
        "      box1_y2 = boxes_preds[...,3:4]\n",
        "\n",
        "\n",
        "      box2_x1 = boxes_labels[...,0:1]\n",
        "      box2_y1 = boxes_labels[...,1:2]\n",
        "      box2_x2 = boxes_labels[...,2:3]\n",
        "      box2_y2 = boxes_labels[...,3:4]\n",
        "  x1 = torch.max(box1_x1,box2_x1)\n",
        "  y1 = torch.max(box1_y1,box2_y1)\n",
        "  x2 = torch.min(box1_x2,box2_x2)\n",
        "  y2 = torch.min(box1_y2,box2_y2)\n",
        "\n",
        "  intersection = (x2-x1).clamp(0)*(y2-y1).clamp(0)\n",
        "\n",
        "  box1_area = abs((box1_x2-box1_x1)*(box1_y2-box1_y1))\n",
        "  box2_area = abs((box2_x2-box2_x1)*(box2_y2-box2_y1))\n",
        "\n",
        "  return intersection/(box1_area+box2_area-intersection+1e-6)"
      ],
      "metadata": {
        "id": "-WAGOwdPhUhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression(bboxes,iou_threshold,threshold,box_format=\"corners\"):\n",
        "\n",
        "  assert type(bboxes) == list\n",
        "\n",
        "  bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "  bboxes = sorted(bboxes,key=lambda x:x[1],reverse=True)\n",
        "  bboxes_after_nms = []\n",
        "\n",
        "  while bboxes:\n",
        "    chosen_box = bboxes.pop(0)\n",
        "\n",
        "    bboxes = [\n",
        "        box for box in bboxes if box[0] != chosen_box[0] or intersection_over_union(torch.tensor(chosen_box[2:]),torch.tensor(box[2:]),box_format=box_format) < iou_threshold\n",
        "    ]\n",
        "\n",
        "    bboxes_after_nms.append(chosen_box)\n",
        "  return bboxes_after_nms"
      ],
      "metadata": {
        "id": "jt5ji8t7iHr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(pred_boxes,true_boxes,iou_threshold=0.5,box_format=\"midpoint\",num_classes=20):\n",
        "\n",
        "  average_precisions = []\n",
        "  epsilon = 1e-6\n",
        "\n",
        "  for c in range(num_classes):\n",
        "    detections = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for detection in pred_boxes:\n",
        "      if detection[1] == c:\n",
        "        detections.append(detection)\n",
        "    for true_box in true_boxes:\n",
        "      if true_box[1] == c:\n",
        "        ground_truths.append(true_box)\n",
        "    amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "    for key,val in amount_bboxes.items():\n",
        "      amount_bboxes[key] = torch.zeros(val)\n",
        "    detections.sort(key=lambda x:x[2],reverse=True)\n",
        "    TP = torch.zeros((len(detections)))\n",
        "    FP = torch.zeros((len(detections)))\n",
        "    total_true_bboxes = len(ground_truths)\n",
        "\n",
        "    if total_true_bboxes == 0:\n",
        "      continue\n",
        "    for detection_idx,detection in enumerate(detections):\n",
        "      ground_truth_img = [\n",
        "          bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "      ]\n",
        "      num_gts = len(ground_truth_img)\n",
        "      best_iou = 0\n",
        "\n",
        "      for idx,gt in enumerate(ground_truth_img):\n",
        "        iou = intersection_over_union(torch.tensor(detection[3:]),torch.tensor(gt[3:]),box_format=box_format)\n",
        "        if iou > best_iou:\n",
        "          best_iou = iou\n",
        "          best_gt_idx = idx\n",
        "      if best_iou > iou_threshold:\n",
        "        if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "\n",
        "          TP[detection_idx] = 1\n",
        "          amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "        else:\n",
        "          FP[detection_idx] = 1\n",
        "      else:\n",
        "        FP[detection_idx] = 1\n",
        "    TP_cumsum = torch.cumsum(TP,dim=0)\n",
        "    FP_cumsum = torch.cumsum(FP,dim=0)\n",
        "    recalls = TP_cumsum/(total_true_bboxes+epsilon)\n",
        "    precisions = torch.divide(TP_cumsum,(TP_cumsum+FP_cumsum+epsilon))\n",
        "    precisions = torch.cat((torch.tensor([1]),precisions))\n",
        "    recalls = torch.cat((torch.tensor([0]),recalls))\n",
        "\n",
        "    average_precisions.append(torch.trapz(precisions,recalls))\n",
        "  return sum(average_precisions)/len(average_precisions)"
      ],
      "metadata": {
        "id": "DhqXvM0Piu5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_config = [\n",
        "    (7,64,2,3),\n",
        "    \"M\",\n",
        "    (3,192,1,1),\n",
        "    \"M\",\n",
        "    (1,128,1,0),\n",
        "    (3,256,1,1),\n",
        "    (1,256,1,0),\n",
        "    (3,512,1,1),\n",
        "    \"M\",\n",
        "    [(1,256,1,0),(3,512,1,1),4],\n",
        "    (1,512,1,0),\n",
        "    (3,1024,1,1),\n",
        "    \"M\",\n",
        "    [(1,512,1,0),(3,1024,1,1),2],\n",
        "    (3,1024,1,1),\n",
        "    (3,1024,2,1),\n",
        "    (3,1024,1,1),\n",
        "    (3,1024,1,1)\n",
        "]\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,**kwargs):\n",
        "    super(CNNBlock,self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels,out_channels,bias=False,**kwargs)\n",
        "    self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "    self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "  def forward(self,x):\n",
        "    return self.leakyrely(self.batchnorm(self.conv(x)))\n",
        "class Yolov1(nn.Module):\n",
        "  def __init__(self,in_channels=3,**kwargs):\n",
        "    super(Yolov1,self).__init__()\n",
        "    self.architecture = architecture_config\n",
        "    self.in_channels = in_channels\n",
        "    self.darknet = self._create_conv_layers(self.architecture)\n",
        "    self.fcs = self._create_fcs(**kwargs)\n",
        "  def forward(self,x):\n",
        "    x = self.darknet(x)\n",
        "    return self.fcs(torch.flatten(x,start_dim = 1))\n",
        "  def _create_conv_layers(self,architecture):\n",
        "    layers = []\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for x in architecture:\n",
        "      if type(x) == tuple:\n",
        "        layers +=[CNNBlock(in_channels,x[1],kernel_size=x[0],stride=x[2],padding=x[3])]\n",
        "        in_channels = x[1]\n",
        "      elif type(x) == str:\n",
        "        layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n",
        "      elif type(x) == list:\n",
        "        conv1 = x[0]\n",
        "        conv2 = x[1]\n",
        "\n",
        "        num_repeats = x[2]\n",
        "\n",
        "        for _ in range(num_repeats):\n",
        "          layers+=[\n",
        "              CNNBlock(in_channels,conv1[1],kernel_size=conv1[0],stride=conv1[2],padding=conv1[3]),\n",
        "          ]\n",
        "          layers+=[CNNBlock(in_channels,conv2[1],kernel_size=conv2[0],stride=conv2[2],padding=conv2[3])]\n",
        "          in_channels = conv2[1]\n",
        "    return nn.Sequential(*layers)\n",
        "  def _create_fcs(self,split_size,num_boxes,_num_classes):\n",
        "    S,B,C = split_size=num_boxes,num_classes\n",
        "\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1024*S*S,4096),\n",
        "        nn.Dropout(0.0),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(4096,S*S*(C+B*5))\n",
        "    )"
      ],
      "metadata": {
        "id": "WGfGTvCYklkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoLoLoss(nn.Module):\n",
        "  def __init__(self,S=7,B=2,C=20):\n",
        "    super(YoLoLoss,self).__init__()\n",
        "    self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.lambda_noobj = 0.5\n",
        "    self.lambda_coord = 5\n",
        "  def forward(self,predictions,target):\n",
        "    predictions= predictions.reshape(-1,self.S,self.S,self.C+self.B*5)\n",
        "\n",
        "    iou_b1 = intersection_over_union(predictions[...,21:25],target[...,21:25])\n",
        "    iou_b2 = intersection_over_union(predictions[...,26:30],target[...,21:25])\n",
        "\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0),iou_b2.unsqueeze(0)],dim=0)\n",
        "\n",
        "    iou_maxes,bestbox = torch.max(ious,dim=0)\n",
        "    exists_box = target[...,20].unsqueeze(3)\n",
        "\n",
        "    box_predictions = exists_box*(\n",
        "        (\n",
        "            bestbox*predictions[...,26:30]+(1-bestbox)*predictions[...,21:25]\n",
        "        )\n",
        "    )\n",
        "    box_targets = exists_box*target[...,21:25]\n",
        "\n",
        "    box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4]+1e-6))\n",
        "    box_targets[...,2:4] = torch.sqrt(box_targets[...,2:4])\n",
        "\n",
        "    box_loss = self.mse(torch.flatten(box_predictions,end_dim=-2),torch.flatten(box_targets,end_dim=-2))\n",
        "\n",
        "\n",
        "    pred_box = (bestbox*predictions[...,25:26]+1(1-bestbox)*predictions[...,20:21])\n",
        "    object_loss = self.mse(\n",
        "        torch.flatten(exists_box*pred_box),\n",
        "        torch.flatten(exists_box*target[...,20:21])\n",
        "    )\n",
        "\n",
        "    no_object_loss = self.mse(\n",
        "        torch.flatten((1-exists_box)*predictions[...,20:21],start_dim=1),\n",
        "        torch.flatten((1-exists_box)*target[...,20:21],start_dim=1)\n",
        "    )\n",
        "    no_object_loss +=self.mse(\n",
        "        torch.flatten((1-exists_box)*predictions[...,25:26],start_dim=1),\n",
        "        torch.flatten((1-exists_box)*target[...,20:21],start_dim=1)\n",
        "    )\n",
        "\n",
        "    class_loss = self.mse(\n",
        "        torch.flatten(exists_box*predictions[...,:20],end_dim=-2),\n",
        "        torch.flatten(exists_box*target[...,:20],end_dim=-2)\n",
        "\n",
        "    )\n",
        "    loss = (self.lambda_coord*box_loss + object_loss + self.lambda_noobj*no_object_loss+class_loss)\n",
        "    return loss\n",
        ""
      ],
      "metadata": {
        "id": "2A5CheRNmnxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "Lr = 2e-5\n",
        "device = \"cuda\"\n",
        "\n",
        "batch_size=16\n",
        "EPOCHS = 300\n",
        "\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "PIN_MEMORY = True\n",
        "\n",
        "LOAD_MODEL = False\n",
        "\n",
        "LOAD_MODEL_FILE = \"yolov1.pth.tar\""
      ],
      "metadata": {
        "id": "p9HiuvvmoSpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH = 448\n",
        "HEIGHT = 448\n",
        "\n",
        "def get_train_transforms():\n",
        "  return A.Compose([\n",
        "      A.OneOf([\n",
        "          A.HueSaturationValue(hue_shift_limit=0.2,sat_shift_limit=0.2,val_shift_limit=0.2,p=0.9),\n",
        "          A.RandomBrightnessContrast(brightness_limit=0.2,contrast_limit=0.2,p=0.9)\n",
        "      ],p=0.9),\n",
        "      A.ToGray(p=0.01),\n",
        "      A.HorizontalFlip(p=0.2),\n",
        "      A.VerticalFlip(p=0.2),\n",
        "      A.Resize(height=HEIGHT,width=WIDTH,p=1),\n",
        "      ToTensorV2(p=1.0)\n",
        "  ],p=1.0,bbox_params=A.BboxParams(format=\"yolo\",min_area=0,min_visibility=0,label_fields=['labels']))\n",
        "def get_valid_transforms():\n",
        "  return A.Compose([\n",
        "      A.Resize(height=HEIGHT,width=WIDTH,p=1.0),\n",
        "      ToTensorV2(p=1.0)\n",
        "  ],bbox_params=A.BboxParams(format=\"yolo\",min_area=0,min_visibility=0,label_fields=['labels']))"
      ],
      "metadata": {
        "id": "N9Qrs2fDof8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {\n",
        "'aeroplane ': 0 ,\n",
        "'bicycle ': 1 ,\n",
        " 'bird ': 2 ,\n",
        " 'boat ': 3 ,\n",
        " 'bottle ': 4 ,\n",
        " 'bus ': 5 ,\n",
        " 'car ': 6 ,\n",
        " 'cat ': 7 ,\n",
        " 'chair ': 8 ,\n",
        " 'cow ': 9 ,\n",
        " ' diningtable ': 10 ,\n",
        " 'dog ': 11 ,\n",
        " 'horse ': 12 ,\n",
        " 'motorbike ': 13 ,\n",
        " 'person ': 14 ,\n",
        " ' pottedplant ': 15 ,\n",
        " 'sheep ': 16 ,\n",
        " 'sofa ': 17 ,\n",
        " 'train ': 18 ,\n",
        " 'tvmonitor ': 19\n",
        " }\n"
      ],
      "metadata": {
        "id": "cYEwjEHEph8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader,model,optimizer,loss_fn,epoch):\n",
        "  mean_loss = []\n",
        "  mean_mAP = []\n",
        "\n",
        "  total_batches = len(train_loader)\n",
        "  display_interval = total_batches //5\n",
        "\n",
        "  for batch_idx,(x,y) in enumerate(train_loader):\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred_boxes,true_boxes = get_bboxes_training(out,y,iou_threshold=0.5,threshold=0.4)\n",
        "    mAP = mean_average_precision(pred_boxes,true_boxes,iou_threshold=0.5,box_format=\"midpoint\")\n",
        "\n",
        "    mean_loss.append(loss.item())\n",
        "    mean_mAP.append(mAP.item())\n",
        "\n",
        "    if batch_idx % display_interval ==0 or batch_idx == total_batches-1:\n",
        "     print ( f\" Epoch : { epoch :3} \\t Iter : { batch_idx :3}/{ total_batches :3} \\tLoss : { loss . item () :3.10 f} \\t mAP : { mAP. item () :3.10 f}\")\n",
        "  avg_loss = sum(mean_loss)/len(mean_loss)\n",
        "  avg_mAP = sum(mean_mAP)/len(mean_mAP)\n",
        "  print(colored(f\"Test \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\",\"green\"))\n",
        "  model.train()\n",
        "  return avg_mAP\n",
        "def test_fn(test_loader,model,loss_fn,epoch):\n",
        "  model.eval()\n",
        "  mean_loss = []\n",
        "  mean_mAP = []\n",
        "\n",
        "  for batch_idx,(x,y) in enumerate(test_loader):\n",
        "    x,y = x.to(device),y.to(device)\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out,y)\n",
        "    pred_boxes,true_boxes = get_bboxes_training(out,y,iou_threshold=0.5,threshold=0.4)\n",
        "    mAP = mean_average_precision(pred_boxes,true_boxes,iou_threshold=0.5,box_format=\"midpoint\")\n",
        "\n",
        "    mean_loss.append(loss.item())\n",
        "    mean_mAP.append(mAP.item())\n",
        "  avg_loss = sum(mean_loss)/len(mean_loss)\n",
        "  avg_mAP = sum(mean_mAP)/len(mean_mAP)\n",
        "  print(colored(f\"Test \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\",\"green\"))\n",
        "  mode.train()\n",
        "  return avg_mAP"
      ],
      "metadata": {
        "id": "q1A-OrwapxFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def train():\n",
        "    # Initialize model, optimizer, loss\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = YoloLoss()\n",
        "\n",
        "    # Load checkpoint if necessary\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "    # Create the full dataset\n",
        "    train_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='train',\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    train_dataset.init_config_yolo(\n",
        "        class_mapping=class_mapping,\n",
        "        custom_transforms=get_train_transforms()\n",
        "    )\n",
        "\n",
        "    testval_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='val',\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "    testval_dataset.init_config_yolo(\n",
        "        class_mapping=class_mapping,\n",
        "        custom_transforms=get_val_transforms()\n",
        "    )\n",
        "\n",
        "    # Split dataset into train, validation, and test sets using indices\n",
        "    dataset_size = len(testval_dataset)\n",
        "    val_size = int(0.15 * dataset_size)\n",
        "    test_size = dataset_size - val_size\n",
        "\n",
        "    val_indices = list(range(val_size))\n",
        "    test_indices = list(range(val_size, val_size + test_size))\n",
        "\n",
        "    # Create SubsetRandomSamplers\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    # Create DataLoaders using the samplers\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=val_sampler,  # Use the sampler here\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=test_sampler,  # Use the sampler here\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    best_mAP_train = 0\n",
        "    best_mAP_val = 0\n",
        "    best_mAP_test = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n",
        "        val_mAP = test_fn(val_loader, model, loss_fn, epoch)\n",
        "        test_mAP = test_fn(test_loader, model, loss_fn, epoch, is_test=True)\n",
        "\n",
        "        # Update best mAP values\n",
        "        if train_mAP > best_mAP_train:\n",
        "            best_mAP_train = train_mAP\n",
        "        if val_mAP > best_mAP_val:\n",
        "            best_mAP_val = val_mAP\n",
        "\n",
        "        # Save checkpoint when validation mAP improves\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
        "\n",
        "        if test_mAP > best_mAP_test:\n",
        "            best_mAP_test = test_mAP\n"
      ],
      "metadata": {
        "id": "mqhzUeQatsWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image_with_labels(image, ground_truth_boxes, predicted_boxes, class_mapping):\n",
        "    \"\"\"Draw both ground truth and predicted bounding boxes on an image, with labels.\"\"\"\n",
        "\n",
        "    # Inverting the class mapping for easy access to class names based on indices\n",
        "    inverted_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "    # Convert the image to a numpy array and get its dimensions\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create a figure and axis for plotting\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(im)  # Display the image\n",
        "\n",
        "    # Plot each ground truth box in green\n",
        "    for box in ground_truth_boxes:\n",
        "        label_index, box = box[0], box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "\n",
        "        # Create a rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"green\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Retrieve the class name and add it as text to the plot\n",
        "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(\n",
        "            upper_left_x * width,\n",
        "            upper_left_y * height,\n",
        "            class_name,\n",
        "            color=\"white\",\n",
        "            fontsize=12,\n",
        "            bbox=dict(facecolor=\"green\", alpha=0.2)\n",
        "        )\n",
        "\n",
        "    # Plot each predicted box in red\n",
        "    for box in predicted_boxes:\n",
        "        label_index, box = box[0], box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "\n",
        "        # Create a rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"red\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Retrieve the class name and add it as text to the plot\n",
        "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(\n",
        "            upper_left_x * width,\n",
        "            upper_left_y * height,\n",
        "            class_name,\n",
        "            color=\"white\",\n",
        "            fontsize=12,\n",
        "            bbox=dict(facecolor=\"red\", alpha=0.2)\n",
        "        )\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GwIXlKgStljV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    # Create a YOLO model object with specific hyperparameters\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "\n",
        "    # Load saved model weights and optimizer information from a file, if applicable\n",
        "    if LOAD_MODEL:\n",
        "        model.load_state_dict(torch.load(LOAD_MODEL_FILE)['state_dict'])\n",
        "\n",
        "    # Prepare the test dataset and DataLoader for model evaluation\n",
        "    test_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        image_set='val',\n",
        "        download=False\n",
        "    )\n",
        "    test_dataset.init_config_yolo(\n",
        "        class_mapping=class_mapping,\n",
        "        custom_transforms=get_valid_transforms()\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    # Iterate over the test dataset and process each batch\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        out = model(x)\n",
        "\n",
        "        # Convert model output to bounding boxes and apply non-max suppression\n",
        "        pred_bboxes = cellboxes_to_boxes(out)\n",
        "        gt_bboxes = cellboxes_to_boxes(y)\n",
        "\n",
        "        # Plot the first 8 images with their ground truth and predicted bounding boxes\n",
        "        for idx in range(8):\n",
        "            pred_box = non_max_suppression(\n",
        "                pred_bboxes[idx],\n",
        "                iou_threshold=0.5,\n",
        "                threshold=0.4,\n",
        "                box_format=\"midpoint\"\n",
        "            )\n",
        "            gt_box = non_max_suppression(\n",
        "                gt_bboxes[idx],\n",
        "                iou_threshold=0.5,\n",
        "                threshold=0.4,\n",
        "                box_format=\"midpoint\"\n",
        "            )\n",
        "\n",
        "            image = x[idx].permute(1, 2, 0).to(\"cpu\") / 255\n",
        "            plot_image_with_labels(image, gt_box, pred_box, class_mapping)\n",
        "\n",
        "        break  # Stop after processing the first batch\n"
      ],
      "metadata": {
        "id": "zx8O7OPdusfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}